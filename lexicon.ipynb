{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building out a lexicon for news sentiment analysis\n",
    "#### Data from (https://sraf.nd.edu/textual-analysis/resources/#LM%20Sentiment%20Word%20Lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/loughran_dictionary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Sequence Number</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Word Proportion</th>\n",
       "      <th>Average Proportion</th>\n",
       "      <th>Std Dev</th>\n",
       "      <th>Doc Count</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Uncertainty</th>\n",
       "      <th>Litigious</th>\n",
       "      <th>Constraining</th>\n",
       "      <th>Superfluous</th>\n",
       "      <th>Interesting</th>\n",
       "      <th>Modal</th>\n",
       "      <th>Irr_Verb</th>\n",
       "      <th>Harvard_IV</th>\n",
       "      <th>Syllables</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AARDVARK</td>\n",
       "      <td>1</td>\n",
       "      <td>277</td>\n",
       "      <td>1.480000e-08</td>\n",
       "      <td>1.240000e-08</td>\n",
       "      <td>3.560000e-06</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AARDVARKS</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1.600000e-10</td>\n",
       "      <td>9.730000e-12</td>\n",
       "      <td>9.860000e-09</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABACI</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>4.280000e-10</td>\n",
       "      <td>1.390000e-10</td>\n",
       "      <td>6.230000e-08</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABACK</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>6.410000e-10</td>\n",
       "      <td>3.160000e-10</td>\n",
       "      <td>9.380000e-08</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABACUS</td>\n",
       "      <td>5</td>\n",
       "      <td>7250</td>\n",
       "      <td>3.870000e-07</td>\n",
       "      <td>3.680000e-07</td>\n",
       "      <td>3.370000e-05</td>\n",
       "      <td>914</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word  Sequence Number  Word Count  Word Proportion  \\\n",
       "0   AARDVARK                1         277     1.480000e-08   \n",
       "1  AARDVARKS                2           3     1.600000e-10   \n",
       "2      ABACI                3           8     4.280000e-10   \n",
       "3      ABACK                4          12     6.410000e-10   \n",
       "4     ABACUS                5        7250     3.870000e-07   \n",
       "\n",
       "   Average Proportion       Std Dev  Doc Count  Negative  Positive  \\\n",
       "0        1.240000e-08  3.560000e-06         84         0         0   \n",
       "1        9.730000e-12  9.860000e-09          1         0         0   \n",
       "2        1.390000e-10  6.230000e-08          7         0         0   \n",
       "3        3.160000e-10  9.380000e-08         12         0         0   \n",
       "4        3.680000e-07  3.370000e-05        914         0         0   \n",
       "\n",
       "   Uncertainty  Litigious  Constraining  Superfluous  Interesting  Modal  \\\n",
       "0            0          0             0            0            0      0   \n",
       "1            0          0             0            0            0      0   \n",
       "2            0          0             0            0            0      0   \n",
       "3            0          0             0            0            0      0   \n",
       "4            0          0             0            0            0      0   \n",
       "\n",
       "   Irr_Verb  Harvard_IV  Syllables     Source  \n",
       "0         0           0          2  12of12inf  \n",
       "1         0           0          2  12of12inf  \n",
       "2         0           0          3  12of12inf  \n",
       "3         0           0          2  12of12inf  \n",
       "4         0           0          3  12of12inf  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence Number</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Word Proportion</th>\n",
       "      <th>Average Proportion</th>\n",
       "      <th>Std Dev</th>\n",
       "      <th>Doc Count</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Uncertainty</th>\n",
       "      <th>Litigious</th>\n",
       "      <th>Constraining</th>\n",
       "      <th>Superfluous</th>\n",
       "      <th>Interesting</th>\n",
       "      <th>Modal</th>\n",
       "      <th>Irr_Verb</th>\n",
       "      <th>Harvard_IV</th>\n",
       "      <th>Syllables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>86486.000000</td>\n",
       "      <td>8.648600e+04</td>\n",
       "      <td>8.648600e+04</td>\n",
       "      <td>8.648600e+04</td>\n",
       "      <td>8.648600e+04</td>\n",
       "      <td>8.648600e+04</td>\n",
       "      <td>86486.000000</td>\n",
       "      <td>86486.000000</td>\n",
       "      <td>86486.000000</td>\n",
       "      <td>86486.000000</td>\n",
       "      <td>86486.000000</td>\n",
       "      <td>86486.000000</td>\n",
       "      <td>86486.000000</td>\n",
       "      <td>86486.00000</td>\n",
       "      <td>86486.000000</td>\n",
       "      <td>86486.000000</td>\n",
       "      <td>86486.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>43243.500000</td>\n",
       "      <td>2.163537e+05</td>\n",
       "      <td>1.156255e-05</td>\n",
       "      <td>1.156183e-05</td>\n",
       "      <td>1.904424e-05</td>\n",
       "      <td>1.828529e+04</td>\n",
       "      <td>54.706588</td>\n",
       "      <td>8.223192</td>\n",
       "      <td>6.899417</td>\n",
       "      <td>21.004035</td>\n",
       "      <td>4.274819</td>\n",
       "      <td>1.300835</td>\n",
       "      <td>1.579585</td>\n",
       "      <td>0.00148</td>\n",
       "      <td>0.001827</td>\n",
       "      <td>0.073665</td>\n",
       "      <td>2.840934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>24966.502028</td>\n",
       "      <td>7.204730e+06</td>\n",
       "      <td>3.850416e-04</td>\n",
       "      <td>3.893376e-04</td>\n",
       "      <td>1.267700e-04</td>\n",
       "      <td>9.099580e+04</td>\n",
       "      <td>326.982665</td>\n",
       "      <td>128.269555</td>\n",
       "      <td>117.533760</td>\n",
       "      <td>204.367726</td>\n",
       "      <td>92.580984</td>\n",
       "      <td>51.104943</td>\n",
       "      <td>56.310995</td>\n",
       "      <td>0.06062</td>\n",
       "      <td>0.042703</td>\n",
       "      <td>0.344561</td>\n",
       "      <td>1.225666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>21622.250000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>43243.500000</td>\n",
       "      <td>6.300000e+01</td>\n",
       "      <td>3.370000e-09</td>\n",
       "      <td>2.870000e-09</td>\n",
       "      <td>6.260000e-07</td>\n",
       "      <td>4.000000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>64864.750000</td>\n",
       "      <td>1.905000e+03</td>\n",
       "      <td>1.020000e-07</td>\n",
       "      <td>8.640000e-08</td>\n",
       "      <td>5.830000e-06</td>\n",
       "      <td>8.340000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>86486.000000</td>\n",
       "      <td>1.386049e+09</td>\n",
       "      <td>7.407449e-02</td>\n",
       "      <td>7.539358e-02</td>\n",
       "      <td>1.152622e-02</td>\n",
       "      <td>1.028262e+06</td>\n",
       "      <td>2014.000000</td>\n",
       "      <td>2012.000000</td>\n",
       "      <td>2012.000000</td>\n",
       "      <td>2018.000000</td>\n",
       "      <td>2011.000000</td>\n",
       "      <td>2009.000000</td>\n",
       "      <td>2009.000000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sequence Number    Word Count  Word Proportion  Average Proportion  \\\n",
       "count     86486.000000  8.648600e+04     8.648600e+04        8.648600e+04   \n",
       "mean      43243.500000  2.163537e+05     1.156255e-05        1.156183e-05   \n",
       "std       24966.502028  7.204730e+06     3.850416e-04        3.893376e-04   \n",
       "min           1.000000  0.000000e+00     0.000000e+00        0.000000e+00   \n",
       "25%       21622.250000  0.000000e+00     0.000000e+00        0.000000e+00   \n",
       "50%       43243.500000  6.300000e+01     3.370000e-09        2.870000e-09   \n",
       "75%       64864.750000  1.905000e+03     1.020000e-07        8.640000e-08   \n",
       "max       86486.000000  1.386049e+09     7.407449e-02        7.539358e-02   \n",
       "\n",
       "            Std Dev     Doc Count      Negative      Positive   Uncertainty  \\\n",
       "count  8.648600e+04  8.648600e+04  86486.000000  86486.000000  86486.000000   \n",
       "mean   1.904424e-05  1.828529e+04     54.706588      8.223192      6.899417   \n",
       "std    1.267700e-04  9.099580e+04    326.982665    128.269555    117.533760   \n",
       "min    0.000000e+00  0.000000e+00      0.000000      0.000000      0.000000   \n",
       "25%    0.000000e+00  0.000000e+00      0.000000      0.000000      0.000000   \n",
       "50%    6.260000e-07  4.000000e+01      0.000000      0.000000      0.000000   \n",
       "75%    5.830000e-06  8.340000e+02      0.000000      0.000000      0.000000   \n",
       "max    1.152622e-02  1.028262e+06   2014.000000   2012.000000   2012.000000   \n",
       "\n",
       "          Litigious  Constraining   Superfluous   Interesting        Modal  \\\n",
       "count  86486.000000  86486.000000  86486.000000  86486.000000  86486.00000   \n",
       "mean      21.004035      4.274819      1.300835      1.579585      0.00148   \n",
       "std      204.367726     92.580984     51.104943     56.310995      0.06062   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.00000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.00000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.00000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.00000   \n",
       "max     2018.000000   2011.000000   2009.000000   2009.000000      3.00000   \n",
       "\n",
       "           Irr_Verb    Harvard_IV     Syllables  \n",
       "count  86486.000000  86486.000000  86486.000000  \n",
       "mean       0.001827      0.073665      2.840934  \n",
       "std        0.042703      0.344561      1.225666  \n",
       "min        0.000000      0.000000      0.000000  \n",
       "25%        0.000000      0.000000      2.000000  \n",
       "50%        0.000000      0.000000      3.000000  \n",
       "75%        0.000000      0.000000      4.000000  \n",
       "max        1.000000      2.000000      9.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Word                  86486\n",
       "Sequence Number       86486\n",
       "Word Count            63114\n",
       "Word Proportion       63114\n",
       "Average Proportion    63114\n",
       "Std Dev               63114\n",
       "Doc Count             63114\n",
       "Negative               2355\n",
       "Positive                354\n",
       "Uncertainty             297\n",
       "Litigious               904\n",
       "Constraining            184\n",
       "Superfluous              56\n",
       "Interesting              68\n",
       "Modal                    60\n",
       "Irr_Verb                158\n",
       "Harvard_IV             4188\n",
       "Syllables             85970\n",
       "Source                86486\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.astype(bool).sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appears to be only 2355 words labeled as negative and 354 labeled as positive, but their values seem to all be 2009?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(354, 19)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['Positive'] != 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2355, 19)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['Negative'] != 0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loughran Dictionary above gives weird pos/neg sentiment values... (Why 2009???) Going to check out the other sentiment dictionary by loughran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "xls = pd.ExcelFile(\"data/loughran_sentiment_dictionary.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ABUNDANCE', 'ABUNDANT', 'ACCLAIMED', 'ACCOMPLISH', 'ACCOMPLISHED', 'ACCOMPLISHES', 'ACCOMPLISHING', 'ACCOMPLISHMENT', 'ACCOMPLISHMENTS', 'ACHIEVE']\n",
      "353\n"
     ]
    }
   ],
   "source": [
    "pos = pd.read_excel(xls, \"Positive\")\n",
    "positive_words = list(pos[\"ABLE\"])\n",
    "print(positive_words[:10])\n",
    "print(len(positive_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ABANDONED', 'ABANDONING', 'ABANDONMENT', 'ABANDONMENTS', 'ABANDONS', 'ABDICATED', 'ABDICATES', 'ABDICATING', 'ABDICATION', 'ABDICATIONS']\n",
      "2354\n"
     ]
    }
   ],
   "source": [
    "neg = pd.read_excel(xls, \"Negative\")\n",
    "negative_words = list(neg[\"ABANDON\"])\n",
    "print(negative_words[:10])\n",
    "print(len(negative_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data also contains strong/weak modals and uncertain words.. not going to use for now, but could look into this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296 18 26\n"
     ]
    }
   ],
   "source": [
    "unc = pd.read_excel(xls, \"Uncertainty\")\n",
    "strong = pd.read_excel(xls, \"StrongModal\")\n",
    "weak = pd.read_excel(xls, \"WeakModal\")\n",
    "print(unc.size, strong.size, weak.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Following data taken from (http://mpqa.cs.pitt.edu/lexicons/subj_lexicon/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8222, 1)\n"
     ]
    }
   ],
   "source": [
    "subj = pd.read_csv(\"data/subj-clues.csv\")\n",
    "subj.drop(inplace=True, columns=[\"Unnamed: 0\", \"Unnamed: 2\", \"Unnamed: 3\",\t\"Unnamed: 4\", \"Unnamed: 5\",\t\"Unnamed: 6\"])\n",
    "subj.rename(inplace=True, index=str, columns={\"values\": \"subj\"})\n",
    "print(subj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8222, 2)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Functions to parse the values for word and sentiment polarity\n",
    "def word(x):\n",
    "    return x.split(\" \")[2].split(\"=\")[1]\n",
    "def sent(x):\n",
    "    if len(x.split(\" \")) > 5:\n",
    "        if len(x.split(\" \")[5].split(\"=\")) > 1:\n",
    "            return x.split(\" \")[5].split(\"=\")[1]\n",
    "        else:\n",
    "            return \"NaN\"\n",
    "    else:\n",
    "        return \"NaN\"\n",
    "    \n",
    "# Transform dataframe to only have word and associated polarity\n",
    "word_vect = np.vectorize(word)\n",
    "sent_vect = np.vectorize(sent)\n",
    "subj = subj.assign(word=lambda x: word_vect(x.subj))\n",
    "subj = subj.assign(sent=lambda x: sent_vect(x.subj))\n",
    "subj.drop(columns=[\"subj\"], inplace=True)\n",
    "subj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word    8222\n",
       "sent    8222\n",
       "dtype: int64"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subj.astype(bool).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abandoned</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abandonment</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abandon</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abase</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abasement</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word      sent\n",
       "0    abandoned  negative\n",
       "1  abandonment  negative\n",
       "2      abandon  negative\n",
       "3        abase  negative\n",
       "4    abasement  negative"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4911, 2)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subj[subj['sent'].str.contains('negative', regex=False)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2718, 2)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subj[subj['sent'].str.contains('positive', regex=False)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(593, 2)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subj[~(subj['sent'].str.contains('positive', regex=False) | subj['sent'].str.contains('negative', regex=False))].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8222, 2)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subj.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, adding these new words to our list of positive/negative words"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "positive_words += list(subj[subj['sent'].str.contains('positive', regex=False)]['word'])\n",
    "negative_words += list(subj[subj['sent'].str.contains('negative', regex=False)]['word'])\n",
    "print(len(positive_words))\n",
    "print(len(negative_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2485\n",
      "5763\n"
     ]
    }
   ],
   "source": [
    "print(len(set(positive_words)))\n",
    "print(len(set(negative_words)))\n",
    "positive_words = list(set(positive_words))\n",
    "negative_words = list(set(negative_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn all words to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words = [x.lower() for x in positive_words]\n",
    "negative_words = [x.lower() for x in negative_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take top 2500 from each set of words, to even odds between pos/neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_words = positive_words[:2500]\n",
    "neg_words = negative_words[:2500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "I am simply checking whether how many good/bad words are contained in the test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'panic', 'attacks', 'started', 'after', 'Chloe', 'watched', 'a', 'man', 'die', 'She', 'spent', 'the', 'past', 'three', 'and', 'a', 'half', 'weeks', 'in', 'training', 'trying', 'to', 'harden', 'herself', 'against', 'the', 'daily', 'onslaught', 'of', 'disturbing', 'posts', 'the', 'hate', 'speech', 'the', 'violent', 'attacks', 'the', 'graphic', 'pornography', 'In', 'a', 'few', 'more', 'days', 'she', 'will', 'become', 'a', 'full', 'time', 'Facebook', 'content', 'moderator', 'or', 'what', 'the', 'company', 'she', 'works', 'for', 'a', 'professional', 'services', 'vendor', 'named', 'Cognizant', 'opaquely', 'calls', 'a', 'process', 'executive', 'For', 'this', 'portion', 'of', 'her', 'education', 'Chloe', 'will', 'have', 'to', 'moderate', 'a', 'Facebook', 'post', 'in', 'front', 'of', 'her', 'fellow', 'trainees', 'When', 'it', 's', 'her', 'turn', 'she', 'walks']\n",
      "Number of positive words: 304\n",
      "Number of negative words: 189\n"
     ]
    }
   ],
   "source": [
    "fb = \"fb-bad.txt\"\n",
    "fb2 = \"fb-sure-bad.txt\"\n",
    "goog = \"goog-bad.txt\"\n",
    "pix = \"pixel-bad.txt\"\n",
    "uber = \"uber-bad.txt\"\n",
    "\n",
    "test = \"test/\"\n",
    "\n",
    "file = test + fb\n",
    "\n",
    "hehe = [\"The\", \"When\"]\n",
    "\n",
    "with open(file) as f:\n",
    "    data = f.read()\n",
    "    words = re.sub(\"[^\\w]\", \" \",  data).split()\n",
    "    print(words[:100])\n",
    "    print(\"Number of positive words:\", len([c for c in words if c.lower() in pos_words]))    \n",
    "    print(\"Number of negative words:\", len([c for c in words if c.lower() in neg_words]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
