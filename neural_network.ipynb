{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "In this notebook, we use the functions we've written in various notebooks to explore how our models' analysis on news sentiment actually compares with changes in stock prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import compare_sentiment as comp_sent\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Data\n",
    "google = comp_sent.collapse_articles(data_source = \"./data/google-data.pkl\", stockName = \"GOOG\", \n",
    "                      time_before = '2018-09-27', time_after = '2019-05-17')\n",
    "apple = comp_sent.collapse_articles(data_source = \"./data/apple-data.pkl\", stockName = \"AAPL\", \n",
    "                      time_before = '2018-09-27', time_after = '2019-05-17')\n",
    "tesla = comp_sent.collapse_articles(data_source = \"./data/tesla-data.pkl\", stockName = \"TSLA\", \n",
    "                      time_before = '2018-09-27', time_after = '2019-05-17')\n",
    "micro = comp_sent.collapse_articles(data_source = \"./data/Microsoft-data.pkl\", stockName = \"MSFT\", \n",
    "                      time_before = '2018-09-27', time_after = '2019-05-17')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_company = pd.concat([google, apple, tesla, micro])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_company['liststring'] = all_company['liststring'].apply(lambda x : x.replace(\",\" , \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_company['increase'] = (all_company['delta'] >= 0).astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>liststring</th>\n",
       "      <th>delta</th>\n",
       "      <th>increase</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-09-27</th>\n",
       "      <td>MANCHESTER England It was past midnight when J...</td>\n",
       "      <td>5.14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-28</th>\n",
       "      <td>You may have noticed a bold advertisement in T...</td>\n",
       "      <td>8.02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-01</th>\n",
       "      <td>A new sitcom airs on CBS while a new documenta...</td>\n",
       "      <td>-8.93</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-02</th>\n",
       "      <td>Student Athlete a documentary with LeBron Jame...</td>\n",
       "      <td>14.04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-03</th>\n",
       "      <td>Apple opened a routine product launch event la...</td>\n",
       "      <td>-9.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   liststring  delta  increase\n",
       "time                                                                          \n",
       "2018-09-27  MANCHESTER England It was past midnight when J...   5.14         1\n",
       "2018-09-28  You may have noticed a bold advertisement in T...   8.02         1\n",
       "2018-10-01  A new sitcom airs on CBS while a new documenta...  -8.93         0\n",
       "2018-10-02  Student Athlete a documentary with LeBron Jame...  14.04         1\n",
       "2018-10-03  Apple opened a routine product launch event la...  -9.67         0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_company.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_company['liststring'], all_company['increase'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "#Tokenize the words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_company['liststring'])\n",
    "\n",
    "# max_length = max([len(s.split()) for s in all_company['liststring']])\n",
    "max_length = 5000\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "X_train_tokens =  tokenizer.texts_to_sequences(X_train)\n",
    "X_test_tokens = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_tokens, maxlen=max_length, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_tokens, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Summary of the built model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 5000, 100)         7008400   \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 128)               87936     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 7,096,465\n",
      "Trainable params: 7,096,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "#Architecture 1\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=max_length))\n",
    "model.add(GRU(units=128,  dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print('Summary of the built model...')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 363 samples, validate on 180 samples\n",
      "Epoch 1/25\n",
      " - 32s - loss: 0.6907 - acc: 0.5455 - val_loss: 0.6929 - val_acc: 0.5056\n",
      "Epoch 2/25\n",
      " - 32s - loss: 0.6862 - acc: 0.5730 - val_loss: 0.6956 - val_acc: 0.5056\n",
      "Epoch 3/25\n",
      " - 30s - loss: 0.6782 - acc: 0.5730 - val_loss: 0.6984 - val_acc: 0.5056\n",
      "Epoch 4/25\n",
      " - 29s - loss: 0.6721 - acc: 0.5730 - val_loss: 0.7016 - val_acc: 0.5056\n",
      "Epoch 5/25\n",
      " - 33s - loss: 0.6630 - acc: 0.5730 - val_loss: 0.7050 - val_acc: 0.5056\n",
      "Epoch 6/25\n",
      " - 37s - loss: 0.6506 - acc: 0.5813 - val_loss: 0.7076 - val_acc: 0.5111\n",
      "Epoch 7/25\n",
      " - 35s - loss: 0.6351 - acc: 0.6006 - val_loss: 0.7140 - val_acc: 0.5111\n",
      "Epoch 8/25\n",
      " - 37s - loss: 0.6103 - acc: 0.6529 - val_loss: 0.7216 - val_acc: 0.5167\n",
      "Epoch 9/25\n",
      " - 34s - loss: 0.5790 - acc: 0.6997 - val_loss: 0.7429 - val_acc: 0.4833\n",
      "Epoch 10/25\n",
      " - 33s - loss: 0.5426 - acc: 0.6942 - val_loss: 0.7551 - val_acc: 0.5056\n",
      "Epoch 11/25\n",
      " - 34s - loss: 0.5151 - acc: 0.7493 - val_loss: 0.7200 - val_acc: 0.5056\n",
      "Epoch 12/25\n",
      " - 32s - loss: 0.4873 - acc: 0.7521 - val_loss: 0.7168 - val_acc: 0.5056\n",
      "Epoch 13/25\n",
      " - 33s - loss: 0.4452 - acc: 0.7328 - val_loss: 0.7198 - val_acc: 0.4944\n",
      "Epoch 14/25\n",
      " - 31s - loss: 0.4251 - acc: 0.7548 - val_loss: 0.7386 - val_acc: 0.5278\n",
      "Epoch 15/25\n",
      " - 30s - loss: 0.4082 - acc: 0.7686 - val_loss: 0.7764 - val_acc: 0.5167\n",
      "Epoch 16/25\n",
      " - 32s - loss: 0.4027 - acc: 0.7521 - val_loss: 0.8083 - val_acc: 0.4944\n",
      "Epoch 17/25\n",
      " - 31s - loss: 0.4064 - acc: 0.7355 - val_loss: 0.8333 - val_acc: 0.4722\n",
      "Epoch 18/25\n",
      " - 32s - loss: 0.3944 - acc: 0.7328 - val_loss: 0.8531 - val_acc: 0.5056\n",
      "Epoch 19/25\n",
      " - 32s - loss: 0.3950 - acc: 0.7603 - val_loss: 0.8368 - val_acc: 0.5000\n",
      "Epoch 20/25\n",
      " - 32s - loss: 0.3835 - acc: 0.7686 - val_loss: 0.8190 - val_acc: 0.5056\n",
      "Epoch 21/25\n",
      " - 32s - loss: 0.3852 - acc: 0.7466 - val_loss: 0.8272 - val_acc: 0.4889\n",
      "Epoch 22/25\n",
      " - 34s - loss: 0.3933 - acc: 0.7245 - val_loss: 0.8340 - val_acc: 0.5056\n",
      "Epoch 23/25\n",
      " - 33s - loss: 0.3754 - acc: 0.7521 - val_loss: 0.8479 - val_acc: 0.5056\n",
      "Epoch 24/25\n",
      " - 33s - loss: 0.3907 - acc: 0.7493 - val_loss: 0.8653 - val_acc: 0.5167\n",
      "Epoch 25/25\n",
      " - 31s - loss: 0.3734 - acc: 0.7658 - val_loss: 0.8773 - val_acc: 0.5167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12eb1ad30>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_pad, y_train, batch_size=128, epochs=25, validation_data=(X_test_pad, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 5000, 100)         7008400   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 4996, 128)         64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 2498, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 319744)            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 319745    \n",
      "=================================================================\n",
      "Total params: 7,392,273\n",
      "Trainable params: 383,873\n",
      "Non-trainable params: 7,008,400\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 363 samples, validate on 180 samples\n",
      "Epoch 1/25\n",
      " - 5s - loss: 1.5137 - acc: 0.5234 - val_loss: 0.9399 - val_acc: 0.4944\n",
      "Epoch 2/25\n",
      " - 5s - loss: 1.1572 - acc: 0.4270 - val_loss: 0.7106 - val_acc: 0.5000\n",
      "Epoch 3/25\n",
      " - 5s - loss: 0.8233 - acc: 0.5730 - val_loss: 1.1358 - val_acc: 0.5056\n",
      "Epoch 4/25\n",
      " - 5s - loss: 0.7482 - acc: 0.5730 - val_loss: 0.6962 - val_acc: 0.5000\n",
      "Epoch 5/25\n",
      " - 5s - loss: 0.7151 - acc: 0.4738 - val_loss: 0.7370 - val_acc: 0.4944\n",
      "Epoch 6/25\n",
      " - 5s - loss: 0.6127 - acc: 0.6860 - val_loss: 0.7826 - val_acc: 0.5056\n",
      "Epoch 7/25\n",
      " - 5s - loss: 0.6104 - acc: 0.5730 - val_loss: 0.8419 - val_acc: 0.5056\n",
      "Epoch 8/25\n",
      " - 5s - loss: 0.5604 - acc: 0.5895 - val_loss: 0.6971 - val_acc: 0.5167\n",
      "Epoch 9/25\n",
      " - 5s - loss: 0.5222 - acc: 0.9917 - val_loss: 0.6948 - val_acc: 0.5167\n",
      "Epoch 10/25\n",
      " - 5s - loss: 0.5114 - acc: 0.9807 - val_loss: 0.6947 - val_acc: 0.5222\n",
      "Epoch 11/25\n",
      " - 5s - loss: 0.4585 - acc: 0.9366 - val_loss: 0.7507 - val_acc: 0.5056\n",
      "Epoch 12/25\n",
      " - 5s - loss: 0.4426 - acc: 0.8512 - val_loss: 0.7247 - val_acc: 0.5111\n",
      "Epoch 13/25\n",
      " - 5s - loss: 0.4035 - acc: 0.9725 - val_loss: 0.6887 - val_acc: 0.5444\n",
      "Epoch 14/25\n",
      " - 5s - loss: 0.3770 - acc: 0.9862 - val_loss: 0.6907 - val_acc: 0.5278\n",
      "Epoch 15/25\n",
      " - 5s - loss: 0.3422 - acc: 0.9807 - val_loss: 0.7253 - val_acc: 0.5167\n",
      "Epoch 16/25\n",
      " - 5s - loss: 0.3196 - acc: 0.9697 - val_loss: 0.7254 - val_acc: 0.5167\n",
      "Epoch 17/25\n",
      " - 5s - loss: 0.2889 - acc: 0.9835 - val_loss: 0.6843 - val_acc: 0.5389\n",
      "Epoch 18/25\n",
      " - 5s - loss: 0.2609 - acc: 0.9917 - val_loss: 0.7255 - val_acc: 0.5222\n",
      "Epoch 19/25\n",
      " - 5s - loss: 0.2345 - acc: 0.9835 - val_loss: 0.7286 - val_acc: 0.5222\n",
      "Epoch 20/25\n",
      " - 5s - loss: 0.2135 - acc: 0.9972 - val_loss: 0.6889 - val_acc: 0.5222\n",
      "Epoch 21/25\n",
      " - 5s - loss: 0.1882 - acc: 1.0000 - val_loss: 0.7563 - val_acc: 0.5167\n",
      "Epoch 22/25\n",
      " - 5s - loss: 0.1706 - acc: 0.9972 - val_loss: 0.7184 - val_acc: 0.5222\n",
      "Epoch 23/25\n",
      " - 5s - loss: 0.1506 - acc: 0.9972 - val_loss: 0.6980 - val_acc: 0.5333\n",
      "Epoch 24/25\n",
      " - 5s - loss: 0.1365 - acc: 0.9945 - val_loss: 0.7489 - val_acc: 0.5222\n",
      "Epoch 25/25\n",
      " - 5s - loss: 0.1221 - acc: 0.9945 - val_loss: 0.7282 - val_acc: 0.5167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x135015828>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "\n",
    "#Architecture 2\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=max_length, trainable = False))\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "\n",
    "# compile network\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train_pad, y_train, batch_size=128, epochs=25, validation_data=(X_test_pad, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
